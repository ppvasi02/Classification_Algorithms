{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "edb89e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Specify the URL of the CSV file\n",
    "url = \"https://storm.cis.fordham.edu/~gweiss/classes/cisc5660/data/sms-spam-dataset.csv\"\n",
    "\n",
    "# Read CSV data\n",
    "data = pd.read_csv(url, encoding = \"latin-1\")\n",
    "\n",
    "# Extract Text Data\n",
    "X_data = data['Text']\n",
    "y = data['Class']\n",
    "\n",
    "ngrams = (1, 1)\n",
    "\n",
    "# Create CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=2, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=500)\n",
    "\n",
    "X_data_counts = vectorizer.fit_transform(X_data.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3b67c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
    "X_data_tf = tf_transformer.transform(X_data_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b5dda012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 39)\t1\n",
      "  (0, 173)\t1\n",
      "  (0, 482)\t1\n",
      "  (0, 171)\t1\n",
      "  (0, 458)\t1\n",
      "{'available': 39, 'great': 173, 'world': 482, 'got': 171, 'wat': 458, 'ok': 297, 'lar': 223, 'wif': 470, 'free': 153, 'entry': 139, 'win': 473, 'final': 146, 'text': 401, 'receive': 341, 'question': 334, 'txt': 438, 'rate': 336, 'cs': 101, 'apply': 33, 'dun': 131, 'say': 356, 'early': 133, 'dont': 122, 'think': 410, 'goes': 166, 'hey': 195, 'weeks': 465, 'word': 478, 'id': 208, 'like': 236, 'fun': 158, 'xxx': 488, 'send': 363, '50': 13, 'brother': 62, 'speak': 387, 'set': 367, 'friends': 155, 'network': 284, 'customer': 102, 'selected': 362, 'prize': 330, 'claim': 82, 'code': 86, 'valid': 445, 'hours': 202, 'mobile': 274, 'update': 441, 'latest': 226, 'colour': 89, 'camera': 71, 'im': 210, 'gonna': 169, 'home': 198, 'soon': 385, 'want': 455, 'talk': 398, 'stuff': 394, 'tonight': 426, 'ive': 213, 'today': 419, 'cash': 74, '100': 1, 'pounds': 326, 'cost': 99, '150p': 5, 'day': 107, '16': 7, 'reply': 343, 'urgent': 443, 'won': 476, 'week': 462, 'www': 486, 'net': 283, 'right': 344, 'words': 479, 'thank': 403, 'wont': 477, 'help': 193, 'times': 417, 'date': 106, 'use': 444, 'message': 263, 'http': 205, 'com': 90, 'oh': 296, 'watching': 460, 'remember': 342, 'yes': 493, 'did': 114, 'make': 254, 'fine': 147, 'way': 461, 'feel': 144, 'miss': 270, 'news': 287, 'ur': 442, 'national': 281, 'try': 434, 'going': 168, 'ì_': 498, 'pay': 308, 'da': 103, 'aft': 25, 'finish': 148, 'lunch': 251, 'lor': 245, 'ard': 34, 'alright': 29, 'meet': 261, 'just': 217, 'eat': 135, 'really': 340, 'getting': 162, 'lol': 241, 'bus': 64, 'left': 229, 'dinner': 116, 'love': 249, 'amp': 30, 'car': 72, 'ill': 209, 'let': 232, 'know': 220, 'theres': 407, 'room': 348, 'work': 480, 'does': 118, 'wait': 449, 'thats': 406, 'sure': 395, 'doesnt': 119, 'live': 239, 'yeah': 490, 'till': 415, 'doing': 120, 'tell': 399, 'thanks': 404, 'ringtone': 346, 'uk': 439, 'month': 276, 'yup': 497, 'look': 243, 'msg': 279, '2nd': 12, 'hello': 192, 'hows': 204, 'tomo': 422, 'trying': 435, 'pls': 318, 'wanted': 456, 'weekend': 463, 'forget': 151, 'need': 282, 'sweet': 396, 'tried': 431, 'sms': 384, 'nokia': 292, 'delivery': 111, 'tomorrow': 423, 'hope': 199, 'man': 257, 'lt': 250, 'gt': 174, 'calls': 69, 'messages': 264, 'missed': 271, 'didnt': 115, 'maybe': 258, 'ask': 35, 'bit': 55, 'time': 416, 'saw': 355, 'class': 83, 'half': 183, 'second': 361, 'morning': 277, 'hes': 194, 'place': 315, 'thought': 413, 'best': 51, 'happy': 185, 'sorry': 386, 'new': 286, 'play': 317, 'end': 137, 'yesterday': 494, 'congrats': 95, 'year': 491, 'special': 388, 'later': 225, 'meeting': 262, 'pick': 313, 'pain': 307, 'good': 170, 'took': 427, 'come': 91, 'double': 123, 'check': 80, 'hair': 182, 'said': 353, 'nice': 288, 'mob': 273, 'awarded': 42, 'bonus': 56, 'trip': 432, '1000': 2, 'dis': 117, '18': 8, '10': 0, 'hear': 190, 'comes': 92, 'money': 275, 'finished': 149, 'hi': 196, 'babe': 45, 'wanna': 454, 'waiting': 450, 'cool': 97, 'pa': 306, 'looking': 244, 'job': 214, 'ah': 27, 'stop': 392, 'real': 339, 'yo': 495, 'started': 390, 'came': 70, 'bed': 49, 'wen': 466, 'don': 121, 'close': 84, 'll': 240, 'night': 289, 'late': 224, 'afternoon': 26, 'means': 260, 'havent': 187, 'smile': 381, 'smiling': 382, 'service': 365, 'guaranteed': 175, '5000': 15, 'buy': 66, 'movie': 278, 'abt': 21, 'wk': 475, 'run': 350, 'forgot': 152, 'cause': 75, 'okay': 298, 'price': 327, 'long': 242, 'driving': 128, 'test': 400, 'youre': 496, 'mean': 259, 'guess': 177, 'says': 358, 'life': 235, 'lot': 247, 'dear': 109, 'birthday': 54, 'making': 256, 'aight': 28, 'address': 24, 'isnt': 212, 'old': 300, 'people': 309, 'better': 52, 'worry': 483, 'busy': 65, 'cos': 98, 'things': 409, 'contact': 96, 'draw': 124, 'shows': 374, '150ppm': 6, 'juz': 218, 'haha': 181, 'went': 467, 'holiday': 197, 'min': 265, 'school': 360, 'food': 150, 'private': 329, 'account': 22, 'points': 323, '2000': 10, 'landline': 222, 'todays': 420, 'award': 41, 'ìï': 499, 'sent': 364, 'girl': 164, 'answer': 32, 'join': 215, 'bt': 63, 'haf': 180, 'chat': 79, 'services': 366, 'sir': 376, 'mail': 253, 'little': 238, 'coz': 100, 'gud': 176, 'ya': 489, 'whats': 468, 'taking': 397, 'sexy': 368, 'luv': 252, 'thk': 412, 'lots': 248, 'tv': 437, 'leaving': 228, 'house': 203, 'boy': 60, 'missing': 272, 'years': 492, 'friend': 154, 'order': 304, 'liao': 234, 'coming': 93, 'believe': 50, 'til': 414, 'smoke': 383, 'worth': 484, 'offer': 294, 'gr8': 172, 'guys': 179, 'working': 481, 'awesome': 44, 'minute': 268, 'xmas': 487, 'jus': 216, 'sis': 377, 'touch': 429, 'able': 20, 'hav': 186, 'story': 393, 'tmr': 418, 'evening': 141, 'decimal': 110, 'sleeping': 380, 'dat': 105, 'oso': 305, 'big': 53, 'ready': 338, 'noe': 291, 'leh': 230, 'easy': 134, 'called': 67, 'important': 211, 'shop': 372, 'happen': 184, 'nite': 290, '500': 14, 'collect': 87, 'start': 389, 'company': 94, 'po': 322, 'walk': 452, '10p': 3, 'wil': 472, 'reach': 337, 'person': 310, 'told': 421, 'face': 142, 'watch': 459, 'thanx': 405, 'asked': 36, 'wake': 451, 'sleep': 379, 'vouchers': 448, 'music': 280, 'angry': 31, 'wid': 469, 'true': 433, 'care': 73, 'video': 446, '750': 16, 'mins': 267, 'shopping': 373, 'ring': 345, 'hot': 200, 'unsubscribe': 440, 'plan': 316, 'club': 85, 'charge': 78, '86688': 19, 'okie': 299, 'lets': 233, 'baby': 46, 'hour': 201, 'phone': 311, 'hurt': 207, 'plz': 320, 'shit': 371, 'book': 57, 'friendship': 156, 'games': 160, 'tones': 425, 'sister': 378, 'weekly': 464, 'box': 59, 'wot': 485, 'dunno': 132, 'dude': 130, 'problem': 332, 'line': 237, 'number': 293, 'chance': 76, 'calling': 68, 'post': 325, 'texts': 402, 'minutes': 269, 'row': 349, 'chikku': 81, 'orange': 303, 'wish': 474, 'quite': 335, 'leave': 227, 'having': 188, 'huh': 206, 'sat': 354, 'office': 295, 'days': 108, 'actually': 23, 'god': 165, 'change': 77, 'poly': 324, 'tone': 424, '1st': 9, 'pm': 321, 'stay': 391, 'drink': 126, 'thing': 408, 'den': 112, 'bring': 61, '250': 11, 'head': 189, 'heart': 191, 'eve': 140, 'land': 221, 'mind': 266, 'tot': 428, 'beautiful': 48, 'bad': 47, 'feeling': 145, 'opt': 302, 'princess': 328, 'enjoy': 138, 'sae': 352, 'details': 113, 'thinking': 411, 'lose': 246, 'bored': 58, 'simple': 375, 'shall': 369, 'attempt': 37, '150': 4, '800': 17, 'kiss': 219, 'probably': 331, 'online': 301, 'pics': 314, 'fuck': 157, 'sch': 359, 'wife': 471, 'ts': 436, 'neva': 285, 'game': 159, 'family': 143, 'pub': 333, 'voucher': 447, 'dad': 104, 'email': 136, 'town': 430, 'saying': 357, 'away': 43, 'wants': 457, 'rite': 347, 'gd': 161, 'makes': 255, 'goin': 167, 'drive': 127, 'guy': 178, 'shes': 370, 'gift': 163, '8007': 18, 'await': 40, 'collection': 88, 'wan': 453, 'drop': 129, 'sad': 351, 'plus': 319, 'pic': 312, 'dreams': 125, 'auction': 38, 'lei': 231}\n"
     ]
    }
   ],
   "source": [
    "print(X_data_counts[0:1])\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7a62166b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10' '100' '1000' '10p' '150' '150p' '150ppm' '16' '18' '1st' '2000'\n",
      " '250' '2nd' '50' '500' '5000' '750' '800' '8007' '86688' 'able' 'abt'\n",
      " 'account' 'actually' 'address' 'aft' 'afternoon' 'ah' 'aight' 'alright'\n",
      " 'amp' 'angry' 'answer' 'apply' 'ard' 'ask' 'asked' 'attempt' 'auction'\n",
      " 'available' 'await' 'award' 'awarded' 'away' 'awesome' 'babe' 'baby'\n",
      " 'bad' 'beautiful' 'bed' 'believe' 'best' 'better' 'big' 'birthday' 'bit'\n",
      " 'bonus' 'book' 'bored' 'box' 'boy' 'bring' 'brother' 'bt' 'bus' 'busy'\n",
      " 'buy' 'called' 'calling' 'calls' 'came' 'camera' 'car' 'care' 'cash'\n",
      " 'cause' 'chance' 'change' 'charge' 'chat' 'check' 'chikku' 'claim'\n",
      " 'class' 'close' 'club' 'code' 'collect' 'collection' 'colour' 'com'\n",
      " 'come' 'comes' 'coming' 'company' 'congrats' 'contact' 'cool' 'cos'\n",
      " 'cost' 'coz' 'cs' 'customer' 'da' 'dad' 'dat' 'date' 'day' 'days' 'dear'\n",
      " 'decimal' 'delivery' 'den' 'details' 'did' 'didnt' 'dinner' 'dis' 'does'\n",
      " 'doesnt' 'doing' 'don' 'dont' 'double' 'draw' 'dreams' 'drink' 'drive'\n",
      " 'driving' 'drop' 'dude' 'dun' 'dunno' 'early' 'easy' 'eat' 'email' 'end'\n",
      " 'enjoy' 'entry' 'eve' 'evening' 'face' 'family' 'feel' 'feeling' 'final'\n",
      " 'fine' 'finish' 'finished' 'food' 'forget' 'forgot' 'free' 'friend'\n",
      " 'friends' 'friendship' 'fuck' 'fun' 'game' 'games' 'gd' 'getting' 'gift'\n",
      " 'girl' 'god' 'goes' 'goin' 'going' 'gonna' 'good' 'got' 'gr8' 'great'\n",
      " 'gt' 'guaranteed' 'gud' 'guess' 'guy' 'guys' 'haf' 'haha' 'hair' 'half'\n",
      " 'happen' 'happy' 'hav' 'havent' 'having' 'head' 'hear' 'heart' 'hello'\n",
      " 'help' 'hes' 'hey' 'hi' 'holiday' 'home' 'hope' 'hot' 'hour' 'hours'\n",
      " 'house' 'hows' 'http' 'huh' 'hurt' 'id' 'ill' 'im' 'important' 'isnt'\n",
      " 'ive' 'job' 'join' 'jus' 'just' 'juz' 'kiss' 'know' 'land' 'landline'\n",
      " 'lar' 'late' 'later' 'latest' 'leave' 'leaving' 'left' 'leh' 'lei' 'let'\n",
      " 'lets' 'liao' 'life' 'like' 'line' 'little' 'live' 'll' 'lol' 'long'\n",
      " 'look' 'looking' 'lor' 'lose' 'lot' 'lots' 'love' 'lt' 'lunch' 'luv'\n",
      " 'mail' 'make' 'makes' 'making' 'man' 'maybe' 'mean' 'means' 'meet'\n",
      " 'meeting' 'message' 'messages' 'min' 'mind' 'mins' 'minute' 'minutes'\n",
      " 'miss' 'missed' 'missing' 'mob' 'mobile' 'money' 'month' 'morning'\n",
      " 'movie' 'msg' 'music' 'national' 'need' 'net' 'network' 'neva' 'new'\n",
      " 'news' 'nice' 'night' 'nite' 'noe' 'nokia' 'number' 'offer' 'office' 'oh'\n",
      " 'ok' 'okay' 'okie' 'old' 'online' 'opt' 'orange' 'order' 'oso' 'pa'\n",
      " 'pain' 'pay' 'people' 'person' 'phone' 'pic' 'pick' 'pics' 'place' 'plan'\n",
      " 'play' 'pls' 'plus' 'plz' 'pm' 'po' 'points' 'poly' 'post' 'pounds'\n",
      " 'price' 'princess' 'private' 'prize' 'probably' 'problem' 'pub'\n",
      " 'question' 'quite' 'rate' 'reach' 'ready' 'real' 'really' 'receive'\n",
      " 'remember' 'reply' 'right' 'ring' 'ringtone' 'rite' 'room' 'row' 'run'\n",
      " 'sad' 'sae' 'said' 'sat' 'saw' 'say' 'saying' 'says' 'sch' 'school'\n",
      " 'second' 'selected' 'send' 'sent' 'service' 'services' 'set' 'sexy'\n",
      " 'shall' 'shes' 'shit' 'shop' 'shopping' 'shows' 'simple' 'sir' 'sis'\n",
      " 'sister' 'sleep' 'sleeping' 'smile' 'smiling' 'smoke' 'sms' 'soon'\n",
      " 'sorry' 'speak' 'special' 'start' 'started' 'stay' 'stop' 'story' 'stuff'\n",
      " 'sure' 'sweet' 'taking' 'talk' 'tell' 'test' 'text' 'texts' 'thank'\n",
      " 'thanks' 'thanx' 'thats' 'theres' 'thing' 'things' 'think' 'thinking'\n",
      " 'thk' 'thought' 'til' 'till' 'time' 'times' 'tmr' 'today' 'todays' 'told'\n",
      " 'tomo' 'tomorrow' 'tone' 'tones' 'tonight' 'took' 'tot' 'touch' 'town'\n",
      " 'tried' 'trip' 'true' 'try' 'trying' 'ts' 'tv' 'txt' 'uk' 'unsubscribe'\n",
      " 'update' 'ur' 'urgent' 'use' 'valid' 'video' 'voucher' 'vouchers' 'wait'\n",
      " 'waiting' 'wake' 'walk' 'wan' 'wanna' 'want' 'wanted' 'wants' 'wat'\n",
      " 'watch' 'watching' 'way' 'week' 'weekend' 'weekly' 'weeks' 'wen' 'went'\n",
      " 'whats' 'wid' 'wif' 'wife' 'wil' 'win' 'wish' 'wk' 'won' 'wont' 'word'\n",
      " 'words' 'work' 'working' 'world' 'worry' 'worth' 'wot' 'www' 'xmas' 'xxx'\n",
      " 'ya' 'yeah' 'year' 'years' 'yes' 'yesterday' 'yo' 'youre' 'yup' 'ì_' 'ìï']\n"
     ]
    }
   ],
   "source": [
    "names_ = vectorizer.get_feature_names_out()\n",
    "print(names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b94620e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data_tf, y, test_size=0.2, random_state=42)\n",
    "classifier = MultinomialNB()   ###\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "96c0102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB with ngrams (1,1), unbalanced.\n",
      "Accuracy: 0.9713004484304932\n",
      "Precision: 0.9538461538461539\n",
      "Recall: 0.8266666666666667\n",
      "Confusion Matrix:\n",
      " [[959   6]\n",
      " [ 26 124]]\n",
      "F1-measure: 0.8857142857142857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score, f1_score\n",
    "print(\"Multinomial NB with ngrams (1,1), unbalanced.\")\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "precision = precision_score(y_test, y_pred, pos_label='spam')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "recall = recall_score(y_test, y_pred, pos_label='spam')\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "confusion_matrix_result = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix_result)\n",
    "\n",
    "# Calculate F1-measure (harmonic mean of precision and recall)\n",
    "f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
    "print(\"F1-measure:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f7bb2aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([210, 297, 217, 250,  91, 174, 122, 171, 170, 220, 209, 236, 442,\n",
       "       416, 168, 198, 386, 103, 249, 455, 245, 282, 107, 225, 196, 419,\n",
       "       410, 363, 498, 195, 114, 461, 296, 399, 318, 406, 493, 289, 344,\n",
       "       423, 241, 109, 490, 313,  35, 120, 356, 173,  45, 199, 311, 480,\n",
       "       254, 213, 458, 353, 261, 185, 395, 277, 263, 340,  66, 147,  30,\n",
       "        83, 169, 489, 401, 115, 404, 467, 232, 275, 462, 426, 288, 449,\n",
       "       181,  93, 408, 421, 235, 315, 364, 293, 499,  98, 227, 270, 262,\n",
       "       385, 497, 279, 496, 224, 162, 453, 144, 153, 286, 379, 474, 413,\n",
       "       203, 477, 495, 187,  97, 192, 204, 131,  73, 444, 328, 309, 450,\n",
       "       191, 214, 251, 394, 155, 434, 398, 468,  55,  28, 188, 412,  72,\n",
       "       247, 338, 332, 376, 148, 342,  52, 116, 331, 257,  27, 176, 460,\n",
       "        80, 348, 135, 409, 223, 260, 403, 371, 152, 316, 405, 459,  64,\n",
       "       234, 240,  46, 435,  53, 242, 491, 108, 295, 310, 369, 306, 266,\n",
       "       354, 388,  23, 430, 105, 150, 485, 392, 343,  70, 358, 265, 118,\n",
       "       193, 179, 258, 157, 418, 229, 177, 133, 194, 337,  54, 154, 104,\n",
       "       218, 243, 299, 298, 201,  47, 190, 396, 138, 165, 219, 335, 128,\n",
       "       308, 381, 463,  77,  20, 206, 269, 479, 482, 417, 276, 186, 494,\n",
       "       216, 454, 230, 307, 142, 355, 166, 470, 360, 290, 127, 400, 253,\n",
       "       238, 208,  51, 132, 481, 231, 437, 141, 167,  21, 380, 119,  62,\n",
       "       252, 389,  50, 451,  61,  67, 189, 149, 130, 301, 357, 178, 117,\n",
       "        49, 164, 272, 112, 256, 184, 158, 411, 259, 136, 291, 488,  36,\n",
       "       492, 387, 271, 183, 456, 378,  29, 182,  43, 483,  26, 255, 317,\n",
       "       143, 427,  24, 457, 161, 300, 233, 414, 370,  65, 393, 110, 278,\n",
       "       145, 228, 126, 397, 428, 202, 180, 390, 359, 373, 333, 320,  60,\n",
       "       367, 415, 244,  44, 377,  57, 471, 391, 472,  25, 345, 422,  34,\n",
       "       339,  39,  81, 129, 137, 361, 433, 100, 452, 407, 113, 212, 248,\n",
       "       121, 466, 372, 156, 305, 432, 350, 151, 334, 239,  79, 197, 140,\n",
       "       285, 125,  22, 375,  94, 347, 134,  75,  48,  68, 384, 382,  63,\n",
       "       469,  90, 319,  92, 264, 211, 215, 207, 246, 429,  31,  69, 351,\n",
       "       159, 321,  58, 368, 438,  84,  32,  76, 402, 325, 172, 268, 487,\n",
       "       106, 478,  74,  95, 274, 420,  99, 102, 383,  12, 312,  96, 475,\n",
       "        78, 283, 314, 287, 200, 123, 237, 267, 294, 341, 441, 124, 163,\n",
       "       431,   9, 304, 323, 362, 473,  87,  59, 443,   0, 465, 284, 303,\n",
       "       226, 327, 349, 292,  38, 280, 365, 374, 326,  13, 484, 222,  33,\n",
       "        89,  71, 322,  37, 336,   7, 486, 447,  41, 329,  86,  10, 160,\n",
       "       366,  40, 111, 146, 446,  18,  17,  16,  19, 464,  15,  14, 436,\n",
       "        11,  88,   8, 205,   6,   5,   4,   3,   2,   1, 324,  85, 352,\n",
       "       281, 425, 476, 302, 445,  42,  56, 221, 175, 424, 439, 346, 139,\n",
       "       330, 273, 440,  82, 448, 101])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.feature_log_prob_[0, :].argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0b344012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words most likely to appear in spam:\n",
      "need: -3.8589\n",
      "bring: -4.1194\n",
      "contact: -4.2767\n",
      "baby: -4.3095\n",
      "tones: -4.3422\n",
      "message: -4.3609\n",
      "eve: -4.4253\n",
      "guy: -4.4420\n",
      "sleeping: -4.4422\n",
      "working: -4.5471\n"
     ]
    }
   ],
   "source": [
    "def topTenWords(vocabulary):\n",
    "    from collections import defaultdict\n",
    "    log_probs = classifier.feature_log_prob_[1, :]\n",
    "  \n",
    "    word_probs = defaultdict(float)\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        word_probs[word] = log_probs[i]\n",
    "\n",
    "\n",
    "    sorted_words = sorted(word_probs.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "\n",
    "    num_to_print = 10\n",
    "    print(\"Top\", num_to_print, \"words most likely to appear in spam:\")\n",
    "    for i in range(num_to_print):\n",
    "        word, prob = sorted_words[i]\n",
    "        print(f\"{word}: {prob:.4f}\")\n",
    "        \n",
    "topTenWords(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b8b4320d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC with ngrams (1,1), unbalanced.\n",
      "Accuracy: 0.97847533632287\n",
      "Precision: 0.9701492537313433\n",
      "Recall: 0.8666666666666667\n",
      "Confusion Matrix:\n",
      " [[961   4]\n",
      " [ 20 130]]\n",
      "F1-measure: 0.915492957746479\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC    ####\n",
    "\n",
    "print(\"SVC with ngrams (1,1), unbalanced.\")\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "precision = precision_score(y_test, y_pred, pos_label='spam')\n",
    "print(\"Precision:\", precision)\n",
    "recall = recall_score(y_test, y_pred, pos_label='spam')\n",
    "print(\"Recall:\", recall)\n",
    "confusion_matrix_result = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix_result)\n",
    "f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
    "print(\"F1-measure:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "196c98bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB with ngrams (1,3), unbalanced.\n",
      "Accuracy: 0.968609865470852\n",
      "Precision: 0.952755905511811\n",
      "Recall: 0.8066666666666666\n",
      "Confusion Matrix:\n",
      " [[959   6]\n",
      " [ 29 121]]\n",
      "F1-measure: 0.8736462093862815\n",
      "Top 10 words most likely to appear in spam:\n",
      "trying: -3.8405\n",
      "den: -4.0941\n",
      "draw: -4.2701\n",
      "hurt: -4.2957\n",
      "weekly: -4.3262\n",
      "date: -4.3617\n",
      "guy: -4.4161\n",
      "heart: -4.4170\n",
      "oso: -4.4325\n",
      "xmas: -4.5618\n"
     ]
    }
   ],
   "source": [
    "ngrams = (1, 3) ####\n",
    "classifier = MultinomialNB()\n",
    "vectorizer = CountVectorizer(min_df=2, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=500)\n",
    "X_data_counts = vectorizer.fit_transform(X_data.values.astype('U'))\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
    "X_data_tf = tf_transformer.transform(X_data_counts)\n",
    "\n",
    "\n",
    "def runExperiment(ngrams):    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data_tf, y, test_size=0.2, random_state=42)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    precision = precision_score(y_test, y_pred, pos_label='spam')\n",
    "    print(\"Precision:\", precision)\n",
    "    recall = recall_score(y_test, y_pred, pos_label='spam')\n",
    "    print(\"Recall:\", recall)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix_result)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
    "    print(\"F1-measure:\", f1)\n",
    "    \n",
    "    \n",
    "print(\"Multinomial NB with ngrams (1,3), unbalanced.\")\n",
    "runExperiment(ngrams)\n",
    "topTenWords(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0760e53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB with ngrams (1,3), SMOTE balanced.\n",
      "Accuracy: 0.9310880829015544\n",
      "Precision: 0.9043824701195219\n",
      "Recall: 0.9608465608465608\n",
      "Confusion Matrix:\n",
      " [[889  96]\n",
      " [ 37 908]]\n",
      "F1-measure: 0.9317598768599281\n",
      "Top 10 words most likely to appear in spam:\n",
      "trying: -3.6754\n",
      "den: -3.9259\n",
      "draw: -4.0641\n",
      "weekly: -4.1022\n",
      "date: -4.1769\n",
      "hurt: -4.1923\n",
      "oso: -4.2674\n",
      "heart: -4.2716\n",
      "xmas: -4.2730\n",
      "guy: -4.2955\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "\n",
    "ngrams = (1, 3) ####\n",
    "classifier = MultinomialNB()\n",
    "vectorizer = CountVectorizer(min_df=2, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=500)\n",
    "X_data_counts = vectorizer.fit_transform(X_data.values.astype('U'))\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
    "X_data_tf = tf_transformer.transform(X_data_counts)\n",
    "X_data_tf, y = smote.fit_resample(X_data_tf, y)  # Reshape for SMOTE\n",
    "\n",
    "print(\"Multinomial NB with ngrams (1,3), SMOTE balanced.\")\n",
    "runExperiment(ngrams)\n",
    "topTenWords(vectorizer.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
