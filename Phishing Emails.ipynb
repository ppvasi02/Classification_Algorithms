{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782420f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"Phishing_Email.csv\", encoding = \"latin-1\")\n",
    "\n",
    "X_data = data['Email Text']\n",
    "y = data['Email Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b356c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09e7f390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Sun, Aug 11, 2002 at 11:17:47AM +0100, wintermute mentioned:\n",
      "> > The impression I get from reading lkml the odd time is\n",
      "> > that IDE has gone downhill since Andre Hedrick was \n",
      "> > effectively removed as maintainer. Martin Dalecki seems\n",
      "> > to have been unable to further development without \n",
      "> > much breakage. \n",
      "> \n",
      "> Hmm... begs the question, why remove Handrick?\n",
      "> If it ain't broke, don't fix it. See, the IDE subsystem is like the One Ring. It's kludginess, due to\n",
      "having to support hundreds of dodgy chipsets & drives means that it is\n",
      "inherintly evil. A few months of looking at the code can turn you sour.\n",
      "Years of looking at it will turn you into an arsehole. They haven't found a hobbit that can code, so mortal humans have to\n",
      "suffice. Kate\n",
      "-- \n",
      "Irish Linux Users' Group: ilug@linux.ie\n",
      "http://www.linux.ie/mailman/listinfo/ilug for (un)subscription information.\n",
      "List maintainer: listmaster@linux.ie\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X_data[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1e719ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):    \n",
    "  if not isinstance(text, str):\n",
    "    text = str(text)  # to string\n",
    "    \n",
    "  number_pattern = r\"\\d+\"\n",
    "  text = re.sub(number_pattern, \"\", text)  # remove numbers\n",
    "    \n",
    "  url_pattern = r\"http[s]?://\\S+\"  # remove URLs\n",
    "  text = re.sub(url_pattern, \"\", text)\n",
    "    \n",
    "  text = text.lower()\n",
    "\n",
    "  text = text.translate(str.maketrans('', '', string.punctuation)) #removes punctuation\n",
    "\n",
    "  stop_words = stopwords.words('english')\n",
    "  text = ' '.join([word for word in text.split() if word not in stop_words]) #removes stopwords\n",
    "\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "  return text\n",
    "\n",
    "clean_X_data = []\n",
    "for text in X_data:\n",
    "  clean_X_data.append(clean_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7a02afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sun aug wintermute mentioned impression get reading lkml odd time ide gone downhill since andre hedrick effectively removed maintainer martin dalecki seems unable development without much breakage hmm begs question remove handrick aint broke dont fix see ide subsystem like one ring kludginess due support hundred dodgy chipsets drive mean inherintly evil month looking code turn sour year looking turn arsehole havent found hobbit code mortal human suffice kate irish linux user group iluglinuxie unsubscription information list maintainer listmasterlinuxie\n"
     ]
    }
   ],
   "source": [
    "print(clean_X_data[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de9e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Email Text'] = clean_X_data\n",
    "data.to_csv(\"Phishing_Email.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43824d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disc uniformitarianism sex lang dick hudson observation u use aughter vocative thoughtprovoking sure fair attribute son treated like senior relative one thing nt normally use brother way aughter hard imagine natural class comprising senior relative excluding brother another seem difference imagining distinction seems senior relative term used wider variety context e g calling distance get someone attention hence beginning utterance whereas seems natural utterance like yes son hand son one like son son help although perhaps latter one completely impossible alexis mr\n"
     ]
    }
   ],
   "source": [
    "print(clean_X_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aaa993eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from collections import Counter\n",
    "\n",
    "def balance_data(X_data, y, target_label=\"Safe Email\", keep_proportion=0.64):\n",
    "  class_counts = Counter(y)\n",
    "  safe_count = class_counts[target_label]\n",
    "\n",
    "  # keep 64% of the Safe Emails\n",
    "  undersampled_data = []\n",
    "  undersampled_labels = []\n",
    "  for i, (data_point, label) in enumerate(zip(X_data, y)):\n",
    "    if label == target_label:\n",
    "      if random() < keep_proportion:\n",
    "        undersampled_data.append(data_point)\n",
    "        undersampled_labels.append(0)\n",
    "    else:\n",
    "      # keep all Phishing Emails\n",
    "      undersampled_data.append(data_point)\n",
    "      undersampled_labels.append(1)\n",
    "\n",
    "  return undersampled_data, undersampled_labels\n",
    "\n",
    "balanced_X, balanced_y = balance_data(clean_X_data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3ea67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ngrams = (1, 1) ####\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=500)\n",
    "X_data_counts = vectorizer.fit_transform(balanced_X)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
    "X_data_tf = tf_transformer.transform(X_data_counts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data_tf, balanced_y, test_size=0.2, random_state=42)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3fdfc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 495)\t0.31536122508848546\n",
      "  (1, 493)\t0.14066204771452306\n",
      "  (1, 474)\t0.16284801364615373\n",
      "  (1, 468)\t0.1485320800529444\n",
      "  (1, 467)\t0.1191579774798663\n",
      "  (1, 464)\t0.17335113816830258\n",
      "  (1, 455)\t0.17568281854700413\n",
      "  (1, 449)\t0.10135120786085484\n",
      "  (1, 433)\t0.1571791952861873\n",
      "  (1, 407)\t0.3289766247587358\n",
      "  (1, 405)\t0.16585965676641837\n",
      "  (1, 353)\t0.1809362349558374\n",
      "  (1, 348)\t0.1770447604482475\n",
      "  (1, 344)\t0.15672852005560317\n",
      "  (1, 306)\t0.1405401168506235\n",
      "  (1, 293)\t0.36013146165208876\n",
      "  (1, 292)\t0.14879081335437697\n",
      "  (1, 290)\t0.1803013497968239\n",
      "  (1, 275)\t0.17355172491585452\n",
      "  (1, 244)\t0.12869163323221472\n",
      "  (1, 232)\t0.10503903406690276\n",
      "  (1, 231)\t0.17120593684375088\n",
      "  (1, 163)\t0.13175926024607654\n",
      "  (1, 158)\t0.16347535925946882\n",
      "  (1, 136)\t0.17409137442512712\n",
      "  :\t:\n",
      "  (11643, 236)\t0.2953981255145133\n",
      "  (11643, 224)\t0.10674200688194156\n",
      "  (11643, 216)\t0.08899974309643179\n",
      "  (11643, 199)\t0.07408513379766374\n",
      "  (11643, 190)\t0.05095213092092492\n",
      "  (11643, 174)\t0.3666312352919519\n",
      "  (11643, 157)\t0.06598939334342813\n",
      "  (11643, 156)\t0.07673277622600265\n",
      "  (11643, 142)\t0.07042835635674785\n",
      "  (11643, 140)\t0.06680173519014604\n",
      "  (11643, 137)\t0.2532174862753353\n",
      "  (11643, 133)\t0.08119349885708069\n",
      "  (11643, 114)\t0.09687281523219858\n",
      "  (11643, 83)\t0.07650050223203708\n",
      "  (11643, 71)\t0.06617993981046966\n",
      "  (11643, 56)\t0.0861031827003449\n",
      "  (11643, 25)\t0.06563832697807852\n",
      "  (11643, 14)\t0.26781046935271285\n",
      "  (11644, 494)\t0.4272726256898285\n",
      "  (11644, 286)\t0.33164356361927594\n",
      "  (11644, 176)\t0.5515450049680102\n",
      "  (11644, 175)\t0.4090605012708598\n",
      "  (11644, 145)\t0.28080974691142774\n",
      "  (11644, 60)\t0.26464047727076073\n",
      "  (11644, 29)\t0.2950077417125547\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5ccf305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for predicted phishing emails (considering Laplace smoothing): ['total', 'id', 'friend', 'energy', 'ect', 'second', 'support', 'present', 'follow', 'fax']\n",
      "Accuracy: 0.9156749404964298\n",
      "Precision: 0.9275158339197748\n",
      "Recall: 0.9008885850991114\n",
      "F1-measure: 0.9140083217753121\n",
      "ROC AUC score: 0.9155999082464434\n",
      "Confusion Matrix:\n",
      " [[1375  103]\n",
      " [ 145 1318]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score, f1_score\n",
    "\n",
    "def topTenWords(vocabulary):\n",
    "    indices = np.where(y_pred == 1)[0]\n",
    "\n",
    "    predicted_phishing_emails = X_test[indices]\n",
    "\n",
    "    alpha = 0.1\n",
    "    \n",
    "    class_log_probs = classifier.predict_log_proba(predicted_phishing_emails)[:, 1]\n",
    "    phishing_weights = [prob - np.log(alpha) for prob in class_log_probs]\n",
    "    phishing_features = vocabulary\n",
    "\n",
    "    sorted_features = sorted(zip(phishing_features, phishing_weights), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_10_phishing_words = [word for word, _ in sorted_features[:10]]\n",
    "\n",
    "    print(\"Top 10 words for predicted phishing emails (considering Laplace smoothing):\", top_10_phishing_words)\n",
    "\n",
    "        \n",
    "        \n",
    "def displayStats():\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    print(\"Precision:\", precision)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    print(\"Recall:\", recall)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(\"F1-measure:\", f1)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    print(\"ROC AUC score:\", roc_auc)\n",
    "    confusion_matrix_result = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix_result)\n",
    "    \n",
    "        \n",
    "topTenWords(vectorizer.vocabulary_)\n",
    "displayStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cd3850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = (1, 5) ####\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=500)\n",
    "X_data_counts = vectorizer.fit_transform(balanced_X)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
    "X_data_tf = tf_transformer.transform(X_data_counts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data_tf, balanced_y, test_size=0.2, random_state=42)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb569c83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for predicted phishing emails (considering Laplace smoothing): ['original message', 'pay', 'opportunity', 'professional', 'click', 'complete', 'follow', 'ect', 'america', 'present']\n",
      "Accuracy: 0.9156749404964298\n",
      "Precision: 0.9245283018867925\n",
      "Recall: 0.9043062200956937\n",
      "F1-measure: 0.9143054595715272\n",
      "ROC AUC score: 0.915617250778564\n",
      "Confusion Matrix:\n",
      " [[1370  108]\n",
      " [ 140 1323]]\n"
     ]
    }
   ],
   "source": [
    "topTenWords(vectorizer.vocabulary_)\n",
    "displayStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0b49e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = (1, 1)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=2000) ####\n",
    "X_data_counts = vectorizer.fit_transform(balanced_X)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
    "X_data_tf = tf_transformer.transform(X_data_counts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data_tf, balanced_y, test_size=0.2, random_state=42)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79758e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for predicted phishing emails (considering Laplace smoothing): ['corp', 'cash', 'functional', 'tm', 'quite', 'difference', 'private', 'reading', 'pre', 'term']\n",
      "Accuracy: 0.9357361441686501\n",
      "Precision: 0.9556509298998569\n",
      "Recall: 0.9131920710868079\n",
      "F1-measure: 0.9339391821041594\n",
      "ROC AUC score: 0.9356217459628897\n",
      "Confusion Matrix:\n",
      " [[1416   62]\n",
      " [ 127 1336]]\n"
     ]
    }
   ],
   "source": [
    "topTenWords(vectorizer.vocabulary_)\n",
    "displayStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ad4b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = (1, 1)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=10, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=500) ####\n",
    "X_data_counts = vectorizer.fit_transform(balanced_X)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
    "X_data_tf = tf_transformer.transform(X_data_counts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data_tf, balanced_y, test_size=0.2, random_state=42)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f52ca3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for predicted phishing emails (considering Laplace smoothing): ['total', 'id', 'friend', 'energy', 'ect', 'second', 'support', 'present', 'follow', 'fax']\n",
      "Accuracy: 0.9156749404964298\n",
      "Precision: 0.9275158339197748\n",
      "Recall: 0.9008885850991114\n",
      "F1-measure: 0.9140083217753121\n",
      "ROC AUC score: 0.9155999082464434\n",
      "Confusion Matrix:\n",
      " [[1375  103]\n",
      " [ 145 1318]]\n"
     ]
    }
   ],
   "source": [
    "topTenWords(vectorizer.vocabulary_)\n",
    "displayStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65322004",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = (1, 1)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=100000) ####\n",
    "X_data_counts = vectorizer.fit_transform(balanced_X)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
    "X_data_tf = tf_transformer.transform(X_data_counts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data_tf, balanced_y, test_size=0.2, random_state=42)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b91c281e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for predicted phishing emails (considering Laplace smoothing): ['release', 'react', 'secret', 'countryside', 'returned', 'possibly', 'asset', 'attempting', 'scheme', 'abu']\n",
      "Accuracy: 0.9554573274396464\n",
      "Precision: 0.9812138728323699\n",
      "Recall: 0.9282296650717703\n",
      "F1-measure: 0.9539866526167895\n",
      "ROC AUC score: 0.9553191627117985\n",
      "Confusion Matrix:\n",
      " [[1452   26]\n",
      " [ 105 1358]]\n"
     ]
    }
   ],
   "source": [
    "topTenWords(vectorizer.vocabulary_)\n",
    "displayStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44fba731",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = (1, 2)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=100000) ####\n",
    "X_data_counts = vectorizer.fit_transform(balanced_X)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
    "X_data_tf = tf_transformer.transform(X_data_counts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data_tf, balanced_y, test_size=0.2, random_state=42)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61853bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for predicted phishing emails (considering Laplace smoothing): ['potential', 'bath', 'occur micro', 'indicating certain', 'paid', 'rx', 'imagination', 'applicant', 'operating', 'half year']\n",
      "Accuracy: 0.9571574294457668\n",
      "Precision: 0.9897435897435898\n",
      "Recall: 0.9234449760765551\n",
      "F1-measure: 0.9554455445544554\n",
      "ROC AUC score: 0.9569863581330003\n",
      "Confusion Matrix:\n",
      " [[1464   14]\n",
      " [ 112 1351]]\n"
     ]
    }
   ],
   "source": [
    "topTenWords(vectorizer.vocabulary_)\n",
    "displayStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f706c802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9585175110506631\n",
      "Precision: 0.9472981987991995\n",
      "Recall: 0.9706083390293917\n",
      "F1-measure: 0.9588116137744767\n",
      "ROC AUC score: 0.9585788650492019\n",
      "Confusion Matrix:\n",
      " [[1399   79]\n",
      " [  43 1420]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC    ####\n",
    "\n",
    "ngrams = (1, 1)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=500) ####\n",
    "X_data_counts = vectorizer.fit_transform(balanced_X)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
    "X_data_tf = tf_transformer.transform(X_data_counts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data_tf, balanced_y, test_size=0.2, random_state=42)\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "displayStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dd5f56f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.976878612716763\n",
      "Precision: 0.9684351914036265\n",
      "Recall: 0.9856459330143541\n",
      "F1-measure: 0.9769647696476964\n",
      "ROC AUC score: 0.9769231018251743\n",
      "Confusion Matrix:\n",
      " [[1431   47]\n",
      " [  21 1442]]\n"
     ]
    }
   ],
   "source": [
    "ngrams = (1, 5)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=100000) ####\n",
    "X_data_counts = vectorizer.fit_transform(balanced_X)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
    "X_data_tf = tf_transformer.transform(X_data_counts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data_tf, balanced_y, test_size=0.2, random_state=42)\n",
    "\n",
    "classifier = SVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "displayStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11eb33c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b0b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"Phishing_Email.csv\", encoding = \"latin-1\")\n",
    "\n",
    "X_data = data['Email Text']\n",
    "y = data['Email Type']\n",
    "\n",
    "import nltk\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):    \n",
    "  if not isinstance(text, str):\n",
    "    text = str(text)  # to string\n",
    "    \n",
    "  number_pattern = r\"\\d+\"\n",
    "  text = re.sub(number_pattern, \"\", text)  # remove numbers\n",
    "    \n",
    "  url_pattern = r\"http[s]?://\\S+\"  # remove URLs\n",
    "  text = re.sub(url_pattern, \"\", text)\n",
    "    \n",
    "  text = text.lower()\n",
    "\n",
    "  text = text.translate(str.maketrans('', '', string.punctuation)) #removes punctuation\n",
    "\n",
    "  stop_words = stopwords.words('english')\n",
    "  text = ' '.join([word for word in text.split() if word not in stop_words]) #removes stopwords\n",
    "\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "  return text\n",
    "\n",
    "clean_X_data = []\n",
    "for text in X_data:\n",
    "  clean_X_data.append(clean_text(text))\n",
    "\n",
    "from random import random\n",
    "from collections import Counter\n",
    "\n",
    "def balance_data(X_data, y, target_label=\"Safe Email\", keep_proportion=0.64):\n",
    "  class_counts = Counter(y)\n",
    "  safe_count = class_counts[target_label]\n",
    "\n",
    "  # keep 64% of the Safe Emails\n",
    "  undersampled_data = []\n",
    "  undersampled_labels = []\n",
    "  for i, (data_point, label) in enumerate(zip(X_data, y)):\n",
    "    if label == target_label:\n",
    "      if random() < keep_proportion:\n",
    "        undersampled_data.append(data_point)\n",
    "        undersampled_labels.append(0)\n",
    "    else:\n",
    "      # keep all Phishing Emails\n",
    "      undersampled_data.append(data_point)\n",
    "      undersampled_labels.append(1)\n",
    "\n",
    "  return undersampled_data, undersampled_labels\n",
    "\n",
    "balanced_X, balanced_y = balance_data(clean_X_data, y)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "ngrams = (1, 1)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=500) ####\n",
    "X_data_counts = vectorizer.fit_transform(balanced_X)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
    "X_data_tf = tf_transformer.transform(X_data_counts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data_tf, balanced_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "#  layers.Dense(8, activation=\"relu\", input_shape=(X_train.shape[1],)),  # First hidden layer with 128 neurons and ReLU activation\n",
    "#  layers.Dense(4, activation=\"relu\"),  # Second hidden layer with 64 neurons and ReLU activation\n",
    "#  layers.Dense(2, activation=\"sigmoid\")  # Output layer with softmax activation (adapt the number of neurons to your number of classes)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "#model_history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on test data (optional)\n",
    "#test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "#print(\"Test accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
