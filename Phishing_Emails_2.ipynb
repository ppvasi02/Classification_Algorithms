{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "782420f1",
      "metadata": {
        "id": "782420f1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv(\"modified_data.csv\", encoding = \"UTF-8\")\n",
        "\n",
        "X_data = data['Email Text']\n",
        "y = data['Email Type']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0b356c18",
      "metadata": {
        "scrolled": true,
        "id": "0b356c18"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "# nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "09e7f390",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09e7f390",
        "outputId": "3bc56378-44a9-4325-cc70-d0289d216c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "disc uniformitarianism sex lang dick hudson observation u use aughter vocative thoughtprovoking sure fair attribute son treated like senior relative one thing nt normally use brother way aughter hard imagine natural class comprising senior relative excluding brother another seem difference imagining distinction seems senior relative term used wider variety context e g calling distance get someone attention hence beginning utterance whereas seems natural utterance like yes son hand son one like son son help although perhaps latter one completely impossible alexis mr\n"
          ]
        }
      ],
      "source": [
        "print(X_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "aaa993eb",
      "metadata": {
        "id": "aaa993eb"
      },
      "outputs": [],
      "source": [
        "from random import random\n",
        "from collections import Counter\n",
        "\n",
        "def balance_data(X_data, y, target_label=\"Safe Email\", keep_proportion=0.64):\n",
        "  class_counts = Counter(y)\n",
        "  safe_count = class_counts[target_label]\n",
        "\n",
        "  # keep 64% of the Safe Emails\n",
        "  undersampled_data = []\n",
        "  undersampled_labels = []\n",
        "  for i, (data_point, label) in enumerate(zip(X_data, y)):\n",
        "    if label == target_label:\n",
        "      if random() < keep_proportion:\n",
        "        undersampled_data.append(data_point)\n",
        "        undersampled_labels.append(0)\n",
        "    else:\n",
        "      # keep all Phishing Emails\n",
        "      undersampled_data.append(data_point)\n",
        "      undersampled_labels.append(1)\n",
        "\n",
        "  return undersampled_data, undersampled_labels\n",
        "\n",
        "balanced_X, balanced_y = balance_data(X_data, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "11eb33c1",
      "metadata": {
        "id": "11eb33c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014ed334-51f8-4474-dc7c-23a15f8ba62d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "#pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "e0b0b1d7",
      "metadata": {
        "id": "e0b0b1d7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv(\"modified_data.csv\", encoding = \"latin-1\")\n",
        "\n",
        "X_data = data['Email Text']\n",
        "y = data['Email Type']\n",
        "\n",
        "import nltk\n",
        "\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "  if not isinstance(text, str):\n",
        "    text = str(text)  # to string\n",
        "\n",
        "  number_pattern = r\"\\d+\"\n",
        "  text = re.sub(number_pattern, \"\", text)  # remove numbers\n",
        "\n",
        "  url_pattern = r\"http[s]?://\\S+\"  # remove URLs\n",
        "  text = re.sub(url_pattern, \"\", text)\n",
        "\n",
        "  text = text.lower()\n",
        "\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation)) #removes punctuation\n",
        "\n",
        "  #stop_words = stopwords.words('english')\n",
        "  #text = ' '.join([word for word in text.split() if word not in stop_words]) #removes stopwords\n",
        "\n",
        "  #lemmatizer = WordNetLemmatizer()\n",
        "  #text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "  return text\n",
        "\n",
        "clean_X_data = []\n",
        "for text in X_data:\n",
        "  clean_X_data.append(clean_text(text))\n",
        "\n",
        "from random import random\n",
        "from collections import Counter\n",
        "\n",
        "def balance_data(X_data, y, target_label=\"Safe Email\", keep_proportion=0.64):\n",
        "  class_counts = Counter(y)\n",
        "  safe_count = class_counts[target_label]\n",
        "\n",
        "  # keep 64% of the Safe Emails\n",
        "  undersampled_data = []\n",
        "  undersampled_labels = []\n",
        "  for i, (data_point, label) in enumerate(zip(X_data, y)):\n",
        "    if label == target_label:\n",
        "      if random() < keep_proportion:\n",
        "        undersampled_data.append(data_point)\n",
        "        undersampled_labels.append(0)\n",
        "    else:\n",
        "      # keep all Phishing Emails\n",
        "      undersampled_data.append(data_point)\n",
        "      undersampled_labels.append(1)\n",
        "\n",
        "  return undersampled_data, undersampled_labels\n",
        "\n",
        "balanced_X, balanced_y = balance_data(clean_X_data, y)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "e3ea0a94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e3ea0a94",
        "outputId": "55ed6f39-cb90-40ab-9a74-3f96d243b51f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 2s 2ms/step - loss: 0.2780 - accuracy: 0.9126\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.1373 - accuracy: 0.9477\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.1235 - accuracy: 0.9520\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.1131 - accuracy: 0.9563\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.1024 - accuracy: 0.9603\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.0897 - accuracy: 0.9671\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 1s 4ms/step - loss: 0.0771 - accuracy: 0.9720\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 1s 4ms/step - loss: 0.0653 - accuracy: 0.9770\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 1s 4ms/step - loss: 0.0553 - accuracy: 0.9808\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.0461 - accuracy: 0.9844\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.0403 - accuracy: 0.9868\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 0.0355 - accuracy: 0.9877\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.0323 - accuracy: 0.9883\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.0302 - accuracy: 0.9890\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.0294 - accuracy: 0.9890\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.0279 - accuracy: 0.9892\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.0280 - accuracy: 0.9892\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.0266 - accuracy: 0.9892\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.0269 - accuracy: 0.9894\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.0260 - accuracy: 0.9897\n",
            "91/91 [==============================] - 0s 2ms/step\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.2848 - accuracy: 0.9372\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-2746b16dc965>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Calculate evaluation metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     96\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[1;32m     97\u001b[0m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "ngrams = (1, 1)\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=2, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=500) ####\n",
        "X_data_counts = vectorizer.fit_transform(balanced_X)\n",
        "\n",
        "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
        "X_data_tf = tf_transformer.transform(X_data_counts)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data_tf, balanced_y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = X_train.toarray()\n",
        "X_test = X_test.toarray()\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)\n",
        "\n",
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "  layers.Dense(64, activation=\"relu\", input_shape=(X_train.shape[1],)),  # First hidden layer with 64 neurons and ReLU activation\n",
        "  layers.Dense(8, activation=\"relu\"),  # Second hidden layer with 8 neurons and ReLU activation\n",
        "  layers.Dense(1, activation=\"sigmoid\")  # Output layer with 1 neuron and sigmoid activation (for binary classification)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model (epochs is the number of training iterations)\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_acc)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"ROC AUC:\", roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "ngrams = (1, 5)\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=2, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=500) ####\n",
        "X_data_counts = vectorizer.fit_transform(balanced_X)\n",
        "\n",
        "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
        "X_data_tf = tf_transformer.transform(X_data_counts)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data_tf, balanced_y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = X_train.toarray()\n",
        "X_test = X_test.toarray()\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)\n",
        "\n",
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "  layers.Dense(64, activation=\"relu\", input_shape=(X_train.shape[1],)),  # First hidden layer with 8 neurons and ReLU activation\n",
        "  layers.Dense(8, activation=\"relu\"),  # Second hidden layer with 4 neurons and ReLU activation\n",
        "  layers.Dense(1, activation=\"sigmoid\")  # Output layer with 1 neuron and sigmoid activation (for binary classification)\n",
        "])\n",
        "\n",
        "# Compile the model (specifying loss function and optimizer)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model (epochs is the number of training iterations)\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "\n",
        "# Evaluate the model on test data (optional)\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYEajgsrOXkw",
        "outputId": "07558729-c710-4156-9585-5ad1eab52e06"
      },
      "id": "rYEajgsrOXkw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "365/365 [==============================] - 2s 2ms/step - loss: 0.2797 - accuracy: 0.8852\n",
            "Epoch 2/20\n",
            "365/365 [==============================] - 1s 2ms/step - loss: 0.1431 - accuracy: 0.9458\n",
            "Epoch 3/20\n",
            "365/365 [==============================] - 1s 2ms/step - loss: 0.1284 - accuracy: 0.9503\n",
            "Epoch 4/20\n",
            "365/365 [==============================] - 1s 2ms/step - loss: 0.1160 - accuracy: 0.9553\n",
            "Epoch 5/20\n",
            "365/365 [==============================] - 1s 2ms/step - loss: 0.1034 - accuracy: 0.9616\n",
            "Epoch 6/20\n",
            "365/365 [==============================] - 1s 2ms/step - loss: 0.0905 - accuracy: 0.9666\n",
            "Epoch 7/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0770 - accuracy: 0.9715\n",
            "Epoch 8/20\n",
            "365/365 [==============================] - 1s 3ms/step - loss: 0.0662 - accuracy: 0.9747\n",
            "Epoch 9/20\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0555 - accuracy: 0.9800\n",
            "Epoch 10/20\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0475 - accuracy: 0.9829\n",
            "Epoch 11/20\n",
            "365/365 [==============================] - 1s 2ms/step - loss: 0.0422 - accuracy: 0.9843\n",
            "Epoch 12/20\n",
            "365/365 [==============================] - 1s 2ms/step - loss: 0.0381 - accuracy: 0.9855\n",
            "Epoch 13/20\n",
            "365/365 [==============================] - 1s 2ms/step - loss: 0.0348 - accuracy: 0.9855\n",
            "Epoch 14/20\n",
            "365/365 [==============================] - 1s 2ms/step - loss: 0.0327 - accuracy: 0.9856\n",
            "Epoch 15/20\n",
            "365/365 [==============================] - 1s 2ms/step - loss: 0.0311 - accuracy: 0.9863\n",
            "Epoch 16/20\n",
            "365/365 [==============================] - 1s 2ms/step - loss: 0.0302 - accuracy: 0.9866\n",
            "Epoch 17/20\n",
            "365/365 [==============================] - 1s 2ms/step - loss: 0.0288 - accuracy: 0.9869\n",
            "Epoch 18/20\n",
            "365/365 [==============================] - 1s 2ms/step - loss: 0.0295 - accuracy: 0.9846\n",
            "Epoch 19/20\n",
            "365/365 [==============================] - 1s 2ms/step - loss: 0.0280 - accuracy: 0.9871\n",
            "Epoch 20/20\n",
            "365/365 [==============================] - 1s 2ms/step - loss: 0.0279 - accuracy: 0.9871\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.1854 - accuracy: 0.9521\n",
            "Test accuracy: 0.9520711898803711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "ngrams = (1, 1)\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=2, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=2000) ####\n",
        "X_data_counts = vectorizer.fit_transform(balanced_X)\n",
        "\n",
        "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
        "X_data_tf = tf_transformer.transform(X_data_counts)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data_tf, balanced_y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = X_train.toarray()\n",
        "X_test = X_test.toarray()\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)\n",
        "\n",
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "  layers.Dense(64, activation=\"relu\", input_shape=(X_train.shape[1],)),  # First hidden layer with 8 neurons and ReLU activation\n",
        "  layers.Dense(8, activation=\"relu\"),  # Second hidden layer with 4 neurons and ReLU activation\n",
        "  layers.Dense(1, activation=\"sigmoid\")  # Output layer with 1 neuron and sigmoid activation (for binary classification)\n",
        "])\n",
        "\n",
        "# Compile the model (specifying loss function and optimizer)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model (epochs is the number of training iterations)\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "\n",
        "# Evaluate the model on test data (optional)\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoA-1ZAZpq7k",
        "outputId": "6d70fc01-420a-4bc5-8c22-34d75e30666e"
      },
      "id": "yoA-1ZAZpq7k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "365/365 [==============================] - 2s 4ms/step - loss: 0.2332 - accuracy: 0.9406\n",
            "Epoch 2/20\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0709 - accuracy: 0.9746\n",
            "Epoch 3/20\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0499 - accuracy: 0.9818\n",
            "Epoch 4/20\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0387 - accuracy: 0.9860\n",
            "Epoch 5/20\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0322 - accuracy: 0.9878\n",
            "Epoch 6/20\n",
            "365/365 [==============================] - 2s 5ms/step - loss: 0.0277 - accuracy: 0.9895\n",
            "Epoch 7/20\n",
            "365/365 [==============================] - 2s 6ms/step - loss: 0.0252 - accuracy: 0.9896\n",
            "Epoch 8/20\n",
            "365/365 [==============================] - 2s 6ms/step - loss: 0.0241 - accuracy: 0.9897\n",
            "Epoch 9/20\n",
            "365/365 [==============================] - 2s 7ms/step - loss: 0.0227 - accuracy: 0.9899\n",
            "Epoch 10/20\n",
            "365/365 [==============================] - 2s 5ms/step - loss: 0.0229 - accuracy: 0.9900\n",
            "Epoch 11/20\n",
            "365/365 [==============================] - 2s 4ms/step - loss: 0.0221 - accuracy: 0.9896\n",
            "Epoch 12/20\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0222 - accuracy: 0.9894\n",
            "Epoch 13/20\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0224 - accuracy: 0.9901\n",
            "Epoch 14/20\n",
            "365/365 [==============================] - 1s 4ms/step - loss: 0.0223 - accuracy: 0.9901\n",
            "Epoch 15/20\n",
            "365/365 [==============================] - 2s 5ms/step - loss: 0.0222 - accuracy: 0.9901\n",
            "Epoch 16/20\n",
            "365/365 [==============================] - 2s 7ms/step - loss: 0.0218 - accuracy: 0.9901\n",
            "Epoch 17/20\n",
            "365/365 [==============================] - 2s 6ms/step - loss: 0.0222 - accuracy: 0.9897\n",
            "Epoch 18/20\n",
            "365/365 [==============================] - 3s 7ms/step - loss: 0.0219 - accuracy: 0.9901\n",
            "Epoch 19/20\n",
            "365/365 [==============================] - 2s 5ms/step - loss: 0.0216 - accuracy: 0.9902\n",
            "Epoch 20/20\n",
            "365/365 [==============================] - 2s 5ms/step - loss: 0.0220 - accuracy: 0.9901\n",
            "92/92 [==============================] - 0s 3ms/step - loss: 0.1871 - accuracy: 0.9617\n",
            "Test accuracy: 0.9616569876670837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQzn7RuMPHAH",
        "outputId": "81f80cb6-35dc-4502-ae25-216c584a17d7"
      },
      "id": "YQzn7RuMPHAH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.04389969 ... 0.         0.         0.        ]\n",
            " [0.05424947 0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "ngrams = (1, 1)\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=2, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=500) ####\n",
        "X_data_counts = vectorizer.fit_transform(balanced_X)\n",
        "\n",
        "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
        "X_data_tf = tf_transformer.transform(X_data_counts)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data_tf, balanced_y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = X_train.toarray()\n",
        "X_test = X_test.toarray()\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)\n",
        "\n",
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "  layers.Dense(64, activation=\"relu\", input_shape=(X_train.shape[1],)),  # First hidden layer with 8 neurons and ReLU activation\n",
        "  layers.Dense(32, activation=\"relu\"),  # Second hidden layer with 32 neurons and ReLU activation\n",
        "  layers.Dense(16, activation=\"relu\"),  # Second hidden layer with 16 neurons and ReLU activation\n",
        "  layers.Dense(8, activation=\"relu\"),  # Second hidden layer with 8 neurons and ReLU activation\n",
        "  layers.Dense(4, activation=\"relu\"),  # Second hidden layer with 4 neurons and ReLU activation\n",
        "  layers.Dense(2, activation=\"relu\"),  # Second hidden layer with 2 neurons and ReLU activation\n",
        "  layers.Dense(1, activation=\"sigmoid\")  # Output layer with 1 neuron and sigmoid activation (for binary classification)\n",
        "])\n",
        "\n",
        "# Compile the model (specifying loss function and optimizer)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model (epochs is the number of training iterations)\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "\n",
        "# Evaluate the model on test data (optional)\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "SR5-6ygFPoNJ",
        "outputId": "ac71aa22-c345-45fa-fe12-d82ff6a06ef1"
      },
      "id": "SR5-6ygFPoNJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'CountVectorizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a3527dec3bfd>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mngrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mX_data_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbalanced_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "ngrams = (1, 5)\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=2, lowercase=True, ngram_range=ngrams, stop_words='english', max_features=2000) ####\n",
        "X_data_counts = vectorizer.fit_transform(balanced_X)\n",
        "\n",
        "tf_transformer = TfidfTransformer(use_idf=True).fit(X_data_counts)\n",
        "X_data_tf = tf_transformer.transform(X_data_counts)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data_tf, balanced_y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = X_train.toarray()\n",
        "X_test = X_test.toarray()\n",
        "y_train = np.array(y_train, dtype=np.float32)\n",
        "y_test = np.array(y_test, dtype=np.float32)\n",
        "\n",
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "  layers.Dense(64, activation=\"relu\", input_shape=(X_train.shape[1],)),  # First hidden layer with 8 neurons and ReLU activation\n",
        "  layers.Dense(32, activation=\"relu\"),  # Second hidden layer with 32 neurons and ReLU activation\n",
        "  layers.Dense(16, activation=\"relu\"),  # Second hidden layer with 16 neurons and ReLU activation\n",
        "  layers.Dense(8, activation=\"relu\"),  # Second hidden layer with 8 neurons and ReLU activation\n",
        "  layers.Dense(4, activation=\"relu\"),  # Second hidden layer with 4 neurons and ReLU activation\n",
        "  layers.Dense(2, activation=\"relu\"),  # Second hidden layer with 2 neurons and ReLU activation\n",
        "  layers.Dense(1, activation=\"sigmoid\")  # Output layer with 1 neuron and sigmoid activation (for binary classification)\n",
        "])\n",
        "\n",
        "# Compile the model (specifying loss function and optimizer)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model (epochs is the number of training iterations)\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
        "\n",
        "# Evaluate the model on test data (optional)\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlY_i-hfxYFN",
        "outputId": "d87a8177-90f4-47e1-b82a-33d30fe77fc9"
      },
      "id": "zlY_i-hfxYFN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "365/365 [==============================] - 3s 5ms/step - loss: 0.3262 - accuracy: 0.8854\n",
            "Epoch 2/20\n",
            "365/365 [==============================] - 2s 4ms/step - loss: 0.0736 - accuracy: 0.9735\n",
            "Epoch 3/20\n",
            "365/365 [==============================] - 2s 4ms/step - loss: 0.0518 - accuracy: 0.9801\n",
            "Epoch 4/20\n",
            "365/365 [==============================] - 2s 4ms/step - loss: 0.0391 - accuracy: 0.9846\n",
            "Epoch 5/20\n",
            "365/365 [==============================] - 3s 7ms/step - loss: 0.0336 - accuracy: 0.9865\n",
            "Epoch 6/20\n",
            "365/365 [==============================] - 3s 7ms/step - loss: 0.0309 - accuracy: 0.9866\n",
            "Epoch 7/20\n",
            "365/365 [==============================] - 2s 4ms/step - loss: 0.0263 - accuracy: 0.9886\n",
            "Epoch 8/20\n",
            "365/365 [==============================] - 2s 4ms/step - loss: 0.0248 - accuracy: 0.9889\n",
            "Epoch 9/20\n",
            "365/365 [==============================] - 2s 4ms/step - loss: 0.0238 - accuracy: 0.9896\n",
            "Epoch 10/20\n",
            "365/365 [==============================] - 2s 4ms/step - loss: 0.0225 - accuracy: 0.9898\n",
            "Epoch 11/20\n",
            "365/365 [==============================] - 2s 5ms/step - loss: 0.0222 - accuracy: 0.9898\n",
            "Epoch 12/20\n",
            "365/365 [==============================] - 2s 5ms/step - loss: 0.0225 - accuracy: 0.9898\n",
            "Epoch 13/20\n",
            "365/365 [==============================] - 3s 7ms/step - loss: 0.0241 - accuracy: 0.9888\n",
            "Epoch 14/20\n",
            "365/365 [==============================] - 4s 10ms/step - loss: 0.0260 - accuracy: 0.9884\n",
            "Epoch 15/20\n",
            "365/365 [==============================] - 3s 9ms/step - loss: 0.0290 - accuracy: 0.9872\n",
            "Epoch 16/20\n",
            "365/365 [==============================] - 3s 9ms/step - loss: 0.0242 - accuracy: 0.9896\n",
            "Epoch 17/20\n",
            "365/365 [==============================] - 5s 13ms/step - loss: 0.0224 - accuracy: 0.9896\n",
            "Epoch 18/20\n",
            "365/365 [==============================] - 4s 11ms/step - loss: 0.0218 - accuracy: 0.9900\n",
            "Epoch 19/20\n",
            "365/365 [==============================] - 2s 5ms/step - loss: 0.0216 - accuracy: 0.9901\n",
            "Epoch 20/20\n",
            "365/365 [==============================] - 2s 5ms/step - loss: 0.0218 - accuracy: 0.9899\n",
            "92/92 [==============================] - 0s 2ms/step - loss: 0.2472 - accuracy: 0.9627\n",
            "Test accuracy: 0.9626840353012085\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}